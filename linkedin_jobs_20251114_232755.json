[
  {
    "job_id": "linkedin_6",
    "source": "linkedin",
    "job_title": "Data Engineer I",
    "company_name": "First Citizens Bank",
    "location": "Texas, United States",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-i-at-first-citizens-bank-4335441427?position=6&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=a0OSENHP4YkMhQk3d36GXg%3D%3D",
    "posted_date": "2025-11-11",
    "scraped_at": "2025-11-15T04:24:52.639240Z",
    "status": "new",
    "job_description": "Overview\n\nThis is a remote role that may only be hired in the following locations: NC, AZ, TX, FL, OH\n\nEnterprise data warehouse supports several critical business functions for the bank including Regulatory Reporting, Finance, Risk steering, and Customer 360. This role is vital for building and maintaining enterprise data platform, data processes, and to support business objectives. Serves as production system support by resolving issues and ensuring ongoing functionality. May oversee the work of less experienced analysts or assist in special projects as needed.\n\nResponsibilities\n\n\nResponsible for designing, building, and maintaining data platform that supports data integrations for Enterprise Data Warehouse, Operational Data Store or Data Marts etc. with appropriate data access, data security, data privacy and data governance.\nEstablish enterprise-scale data integration procedures, data pipelines and frameworks across the data development life cycle. Suggest and implement appropriate technologies to deliver resilient, scalable, and future-proof data solutions.\nCreate data ingestion pipelines in data warehouses and other large-scale data platforms.\nCreating scheduled as well as trigger-based ingestion patterns using scheduling tools.\nCreate performance optimized DDLs for any row-based or columnar databases such as Oracle, Postgres, Netezza database per Logical Data Model.\nPerformance tuning of complex data pipelines and SQL queries.\nPerforms impact analysis of proposed changes on existing architecture, capabilities, system priorities, and technology solutions.\nManage deliverables of developers, perform design reviews and coordinate release management activities.\nEstimate and provide timelines for project activities. Identify, document, and communicate technical risks, issues and alternative solutions discovered during project.\nDrive automation, identify inefficiencies, optimize processes and data flows, and recommend improvements.\nUse agile engineering practices and various data development technologies to rapidly develop and implement efficient data products.\nCollaborate with Product Owners to understand PI goals, PI planning, requirement clarification, and delivery coordination.\nTechnical support for production incidents and failures\nWork with global technology teams across different time zones (primarily US) to deliver timely business value.\n\n\nQualifications\n\nBachelor's Degree and 1 years of experience in Programming OR High School Diploma or GED and 5 years of experience in Programming\n\nPreferred\n\nFunctional Skills:\n\n\nTeam Player: Support peers, team, and department management.\nCommunication: Excellent verbal, written, and interpersonal communication skills.\nProblem Solving: Excellent problem-solving skills, incident management, root cause analysis, and proactive solutions to improve quality.\nPartnership and Collaboration: Develop and maintain partnership with business and IT stakeholders\nAttention to Detail: Ensure accuracy and thoroughness in all tasks.\n\n\nTechnical/Business Skills\n\n\nData Engineering:\nExperience in designing and building Data Warehouse and Data lakes. Good knowledge of data warehouse principles, and concepts.\nTechnical expertise working in large scale Data Warehousing applications and databases such as Oracle, Netezza, Teradata, and SQL Server.\nExperience with public cloud-based data platforms especially Snowflake and AWS.\nData integration skills:\nExpertise in design and development of complex data pipelines\nSolutions using any industry leading ETL tools such as SAP Business Objects Data Services (BODS), Informatica Cloud Data Integration Services (IICS), IBM Data Stage.\nExperience of ELT tools such as DBT, Fivetran, and AWS Glue\nExpert in SQL - development experience in at least one scripting language (Python etc.), adept in tracing and resolving data integrity issues.\nStrong knowledge of data architecture, data design patterns, modeling, and cloud data solutions (Snowflake, AWS Redshift, Google BigQuery).\nData Model: Expertise in Logical and Physical Data Model using Relational or Dimensional Modeling practices, high volume ETL/ELT processes.\nPerformance tuning of data pipelines and DB Objects to deliver optimal performance.\nExperience in Gitlab version control and CI/CD processes.\nExperience working in Financial Industry is a plus.\n\nBenefits are an integral part of total rewards and First Citizens Bank is committed to providing a competitive, thoughtfully designed and quality benefits program to meet the needs of our associates. More information can be found at https://jobs.firstcitizens.com/benefits.\nShow more",
    "skills": [
      "aws",
      "ci/cd",
      "data warehouse",
      "elt",
      "etl",
      "glue",
      "oracle",
      "postgres",
      "python",
      "redshift",
      "sql",
      "sql server"
    ],
    "match_score": 1.0,
    "visa_sponsorship": null,
    "work_mode": "remote"
  },
  {
    "job_id": "linkedin_7",
    "source": "linkedin",
    "job_title": "Data Engineer I",
    "company_name": "First Citizens Bank",
    "location": "Raleigh, NC",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-i-at-first-citizens-bank-4335501251?position=7&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=zp6WVft9Oc8%2B%2F8v5fTej6Q%3D%3D",
    "posted_date": "2025-11-11",
    "scraped_at": "2025-11-15T04:24:52.691047Z",
    "status": "new",
    "job_description": "Overview\n\nThis is a remote role that may only be hired in the following locations: NC, AZ, TX, FL, OH\n\nEnterprise data warehouse supports several critical business functions for the bank including Regulatory Reporting, Finance, Risk steering, and Customer 360. This role is vital for building and maintaining enterprise data platform, data processes, and to support business objectives. Serves as production system support by resolving issues and ensuring ongoing functionality. May oversee the work of less experienced analysts or assist in special projects as needed.\n\nResponsibilities\n\n\nResponsible for designing, building, and maintaining data platform that supports data integrations for Enterprise Data Warehouse, Operational Data Store or Data Marts etc. with appropriate data access, data security, data privacy and data governance.\nEstablish enterprise-scale data integration procedures, data pipelines and frameworks across the data development life cycle. Suggest and implement appropriate technologies to deliver resilient, scalable, and future-proof data solutions.\nCreate data ingestion pipelines in data warehouses and other large-scale data platforms.\nCreating scheduled as well as trigger-based ingestion patterns using scheduling tools.\nCreate performance optimized DDLs for any row-based or columnar databases such as Oracle, Postgres, Netezza database per Logical Data Model.\nPerformance tuning of complex data pipelines and SQL queries.\nPerforms impact analysis of proposed changes on existing architecture, capabilities, system priorities, and technology solutions.\nManage deliverables of developers, perform design reviews and coordinate release management activities.\nEstimate and provide timelines for project activities. Identify, document, and communicate technical risks, issues and alternative solutions discovered during project.\nDrive automation, identify inefficiencies, optimize processes and data flows, and recommend improvements.\nUse agile engineering practices and various data development technologies to rapidly develop and implement efficient data products.\nCollaborate with Product Owners to understand PI goals, PI planning, requirement clarification, and delivery coordination.\nTechnical support for production incidents and failures\nWork with global technology teams across different time zones (primarily US) to deliver timely business value.\n\n\nQualifications\n\nBachelor's Degree and 1 years of experience in Programming OR High School Diploma or GED and 5 years of experience in Programming\n\nPreferred\n\nFunctional Skills:\n\n\nTeam Player: Support peers, team, and department management.\nCommunication: Excellent verbal, written, and interpersonal communication skills.\nProblem Solving: Excellent problem-solving skills, incident management, root cause analysis, and proactive solutions to improve quality.\nPartnership and Collaboration: Develop and maintain partnership with business and IT stakeholders\nAttention to Detail: Ensure accuracy and thoroughness in all tasks.\n\n\nTechnical/Business Skills\n\n\nData Engineering:\nExperience in designing and building Data Warehouse and Data lakes. Good knowledge of data warehouse principles, and concepts.\nTechnical expertise working in large scale Data Warehousing applications and databases such as Oracle, Netezza, Teradata, and SQL Server.\nExperience with public cloud-based data platforms especially Snowflake and AWS.\nData integration skills:\nExpertise in design and development of complex data pipelines\nSolutions using any industry leading ETL tools such as SAP Business Objects Data Services (BODS), Informatica Cloud Data Integration Services (IICS), IBM Data Stage.\nExperience of ELT tools such as DBT, Fivetran, and AWS Glue\nExpert in SQL - development experience in at least one scripting language (Python etc.), adept in tracing and resolving data integrity issues.\nStrong knowledge of data architecture, data design patterns, modeling, and cloud data solutions (Snowflake, AWS Redshift, Google BigQuery).\nData Model: Expertise in Logical and Physical Data Model using Relational or Dimensional Modeling practices, high volume ETL/ELT processes.\nPerformance tuning of data pipelines and DB Objects to deliver optimal performance.\nExperience in Gitlab version control and CI/CD processes.\nExperience working in Financial Industry is a plus.\n\nBenefits are an integral part of total rewards and First Citizens Bank is committed to providing a competitive, thoughtfully designed and quality benefits program to meet the needs of our associates. More information can be found at https://jobs.firstcitizens.com/benefits.\nShow more",
    "skills": [
      "aws",
      "ci/cd",
      "data warehouse",
      "elt",
      "etl",
      "glue",
      "oracle",
      "postgres",
      "python",
      "redshift",
      "sql",
      "sql server"
    ],
    "match_score": 1.0,
    "visa_sponsorship": null,
    "work_mode": "remote"
  },
  {
    "job_id": "linkedin_13",
    "source": "linkedin",
    "job_title": "Data Engineer",
    "company_name": "CallTrackingMetrics",
    "location": "Millersville, MD",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-at-calltrackingmetrics-4321961524?position=13&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=jfo%2FqWkSaPolXVqGI4NFlA%3D%3D",
    "posted_date": "2025-11-10",
    "scraped_at": "2025-11-15T04:24:52.987855Z",
    "status": "new",
    "job_description": "CallTrackingMetrics is a cutting edge software within the SaaS/UCaaS space helping drive revenue for 100,000+ clients in over 90 different countries.\n\nOur Purpose: To create a better human experience through technology\n\nOur Mission: We empower businesses with the tools to transform conversations into an advantage\n\nOur Vision: We revolutionize the ways in which people and businesses connect\n\nWhat do we do?\n\n\nWe help businesses determine which ad campaigns produce the highest return on investment, ultimately driving more conversions, increasing efficiencies, and automating their operations.\nWe help contact centers support and better connect with their customers by offering a variety of tools such as our mobile app, transcriptions, chat and text messaging features, queue tracking, and a bunch of incredible analytics.\nWe help businesses to work remotely as effectively as they do in-person through our soft-phone and contact center support tools, helping thousands of businesses to employ team members around the world.\nWe enable automation through powerful AI features to enhance workforce productivity, discover business opportunities and improve the human experience of communication by reducing the time spent on boring tasks and allowing more human time to interact.\n\n\nOur headquarters is in Maryland, but we have team members around the country focused on helping our customers. We pride ourselves on our amazing culture, which you can learn about here.\n\nWho are we hiring (and what will they do)?\n\nWe are looking for a Data Engineer to join our team.\n\nAs a successful Data Engineer, you will design, maintain, and optimize the data systems that power our integrations, analytics, and reporting. You’ll work across multiple internal and external platforms to ensure our data is accurate, accessible, and actionable.\n\n\nBuild, maintain, and monitor data pipelines and ETL workflows connecting to various external systems (e.g., Salesforce, HubSpot, BigQuery, Snowflake, etc.)\nCollaborate with internal teams to ensure data quality, consistency, and reliability across systems\nSupport the design and implementation of data models and schemas to improve data organization and reporting\nDevelop and maintain internal data reporting infrastructure to support business intelligence, product, and operations teams\nAutomate recurring data processing and transformation tasks\nWork closely with engineering and product teams to improve data availability and accessibility\nTroubleshoot integration failures, data discrepancies, and performance issues in pipelines\n\n\nWhat skills will help you be effective on our team?\n\n\nBachelor’s degree in Computer Science, Data Engineering, Information Systems, or related field\nMinimum 5+ years of experience in data engineering, ETL development, or related roles\nStrong SQL experience (MySQL, PostgreSQL, or another relational database)\nExperience designing and implementing data models and architecture for analytics and reporting\nHands-on experience with external API integrations (Salesforce, HubSpot, BigQuery, Snowflake, etc.)\nExperience building dashboards or reports for internal teams using tools such as Looker, Tableau, Power BI, or SQL-based reporting solutions\nExperience with ETL tools or frameworks (e.g., Airflow, dbt, Pentaho, custom Python-based workflows)\nFamiliarity with data warehousing concepts and cloud data ecosystems (AWS, GCP, etc.)\nKnowledge of scripting languages for data manipulation and automation\nUnderstanding of version control (Git) and CI/CD pipelines for data workflows\nA high-level of accommodation and flexibility to work in a fast-paced environment of constant change and the ability to perform well under pressure and deadlines\nStrong attention to detail and organization\nThe ability to work independently, but also be a strong team player across the organization\nStrong communication skills and the ability to communicate with anyone\n\n\nWhat You Get In Return\n\n\nThe expected salary range for this role may differ based upon the candidate. Salary presented with a job offer will be based on factors such as calibrated job level, educational background, prior work experience, qualifications, skills, competencies, and proficiency for the role.\nWhen we win, you win - we pay quarterly bonuses up to 15% of that quarter’s salary when we reach our revenue goals\nWe love to give out annual merit increases to team members who are meeting our expectations\nWe offer a hybrid in-office work schedule based upon the employee’s position and performance\nGenerous Paid-Time-Off policy that allows employees to accrue up to 30 days per year, depending upon tenure\n8 paid holidays, plus one additional floating holiday employees can use to celebrate their birthday or a federal or religious holiday or their choice\n24 hours of VTO (voluntary time off) allows employees to take paid time off to volunteer at they favorite charities or support their causes\n401k Plan with per-pay employer matching up to 5%, that you are immediately vested in\nFree basic medical, dental, and prescription coverage for employees and generous coverage for your family; plus both HSA and FSA pre-tax account options, which can be used to pay for health related expenses\nUp to 4 weeks of paid parental leave\nFree basic life insurance and AD&D coverage equal to your annual salary\nFree short term disability coverage at 100% of salary after being employed for 6+ months\nFree access to a wide range of health and well-being information and services such as telephonic counseling, legal advice, and will preparation\n\n\nEqual Opportunity Employer/Protected Veterans/Individuals with Disabilities/Affirmative Action\nShow more",
    "skills": [
      "airflow",
      "aws",
      "ci/cd",
      "etl",
      "git",
      "looker",
      "mysql",
      "postgresql",
      "power bi",
      "python",
      "sql",
      "tableau"
    ],
    "match_score": 1.0,
    "visa_sponsorship": null,
    "work_mode": "remote"
  },
  {
    "job_id": "linkedin_5",
    "source": "linkedin",
    "job_title": "Jr. Data Engineer",
    "company_name": "Yum! Brands",
    "location": "Irvine, CA",
    "application_link": "https://www.linkedin.com/jobs/view/jr-data-engineer-at-yum%21-brands-4340070592?position=5&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=WGMBkVCzIXfdC52i88nRAw%3D%3D",
    "posted_date": "2025-11-11",
    "scraped_at": "2025-11-15T04:24:52.588681Z",
    "status": "new",
    "job_description": "This role is hybrid onsite in Irvine, CA. We are unable to provide relocation assistance at this time.*\n\nJoin the dynamic Habit Burger team, part of Yum! Brands, in building industry-leading technology that enhances the experience of both restaurant staff and guests. As a Jr Data Engineer, you’ll play a hands-on role in developing and maintaining data systems that support our mobile app, website, restaurant operations, and integrations with third-party delivery services like DoorDash, Grubhub, and UberEats.\n\nKey Responsibilities\n\n\n\nDevelop and maintain stored procedures, triggers, and other database objects.\nParticipate in report development and system integrations.\nConduct database testing and debugging.\nEngage in code reviews and documentation of database architecture and processes.\nImplement and maintain database security protocols.\nEnsure data integrity, security, and quality.\nMonitor and optimize database performance.\nManage multiple tasks including high-priority requests and ongoing projects.\nCollaborate with developers, stakeholders, and end-users to gather requirements and deliver solutions.\nDesign, develop, and manage scalable and reliable data pipelines.\n\n\n\nRequired Qualifications\n\n\n\nBachelor’s degree in Computer Science or equivalent experience.\n1–3 years of experience in database design, development, and maintenance.\nProficiency in MSSQL (T-SQL, Stored Procedures) or equivalent.\nWorking knowledge of database management systems (e.g., Oracle, MySQL, Microsoft SQL Server).\nFamiliarity with data modeling, normalization, and database design principles.\nUnderstanding of performance tuning, indexing, and query optimization.\nPrior exposure to ETL processes and tools (experience with any ETL tool is a plus).\nComfortable working in IDEs such as Visual Studio and Snowflake.\nExperience with data visualization tools (DOMO preferred; Power BI and Tableau desirable).\n\n\n\nPreferred Qualifications\n\n\n\nKnowledge of SSIS, SSRS, and SSAS\nProgramming experience in Python\nAwareness of data privacy laws and compliance.\nStrong analytical and problem-solving skills.\nExcellent communication and collaboration abilities.\nEagerness to learn and a strong work ethic.\n\n\n\nBenefits\n\n\n\nUp to 4 weeks of vacation annually, plus sick days.\nHybrid work schedule with year-round Flex Day Fridays.\nUp to 10 paid holidays + 1 floating holiday.\nRecharge Days to unplug and reenergize.\nCompetitive bonus program.\nRecognition-based culture.\n\n\n\nFamily Benefits\n\n\n\nComprehensive medical, dental, and vision coverage (including 100% preventative care from Day 1).\nPrescription drug benefits.\nHealthcare and dependent care flexible spending accounts.\nEmployee Assistance Program for employees and dependents.\nGenerous parental leave.\nOnsite childcare through Bright Horizons.\n\n\n\nAdditional Perks\n\n\n\n401(k) plan with 6% company match.\nUp to 2 paid volunteer days annually.\nAccess to LinkedIn Learning.\n\n\n\nSalary Range: $28.99 to $34.03 hourly + bonus eligibility. This is the expected salary range for this position. Ultimately, in determining pay, we'll consider the successful candidate’s location, experience, and other job-related factors.\n\nAt Yum! Brands and at The Habit Restaurants, LLC, one of our core values is to Believe in ALL People. This means seeing the value in everyone and unlocking their full potential to be their best self. The Habit Restaurants, LLC is proud to be an equal opportunity employer and is committed to equity, inclusion, and belonging for all dimensions of diversity. We do not discriminate based on race, color, religion, sex, sexual orientation, gender identity, national origin, veteran status, disability status, age, or any other protected characteristic. We are committed to working with and providing reasonable accommodation to applicants with disabilities or special needs.\n\nUS Job Seekers/Employees - Click here to view the “Know Your Rights” poster and supplement and the Pay Transparency Policy Statement.\n\nEmployment eligibility to work with Yum! in the U.S. is required as the company will not pursue visa sponsorship for this position.\n\nWHO WE ARE?\n\nHABIT stands for Hospitality, Attitude, Be Your Best, Integrity and Teamwork. At the Habit Burger Grill, our people make the difference, which is why since 1969, we’ve prided ourselves on being a people-first culture where each of us are celebrated for bringing our own unique flavor to the table. Our mission is to foster an environment where everyone feels like they are part of The Habit Family through One Habit, One Heart.\n\nThe Habit Burger Grill is a burger-centric, fast-casual restaurant concept that specializes in preparing fresh, cooked-to-order chargrilled burgers and handcrafted sandwiches featuring USDA choice tenderloin beef, grilled chicken and sushi-grade Ahi tuna cooked over an open flame. In addition, it features fresh-cut salads and an appealing selection of sides and shakes. The Habit Burger Grill was named the \"best tasting burger in America\" in July 2014 in a comprehensive survey conducted by one of America's leading consumer magazines.\nShow more",
    "skills": [
      "etl",
      "mysql",
      "oracle",
      "power bi",
      "python",
      "sql",
      "sql server",
      "ssis",
      "ssrs",
      "t-sql",
      "tableau"
    ],
    "match_score": 0.9500000000000001,
    "visa_sponsorship": null,
    "work_mode": "hybrid"
  },
  {
    "job_id": "linkedin_10",
    "source": "linkedin",
    "job_title": "Data Engineer",
    "company_name": "IQVentures",
    "location": "Dublin, OH",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-at-iqventures-4335173523?position=10&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=ix%2Be62MJ2k8lkBu5z%2FUa6Q%3D%3D",
    "posted_date": "2025-11-10",
    "scraped_at": "2025-11-15T04:24:52.839301Z",
    "status": "new",
    "job_description": "Description\n\nPosition Summary:\n\nThe Data Engineer is a technical leader and hands-on developer responsible for designing, building, and optimizing data pipelines and infrastructure to support analytics and reporting. This role will serve as the lead developer on strategic data initiatives, ensuring scalable, high-performance solutions are delivered effectively and efficiently.\n\nThe ideal candidate is self-directed, thrives in a fast-paced project environment, and is comfortable making technical decisions and architectural recommendations. The ideal candidate has prior experience in modern data platforms, most notable Databricks and the “lakehouse” architecture. They will work closely with cross-functional teams, including business stakeholders, data analysts, and engineering teams, to develop data solutions that align with enterprise strategies and business goals.\n\nExperience in the financial industry is a plus, particularly in designing secure and compliant data solutions.\n\nResponsibilities\n\n\nDesign, build, and maintain scalable ETL/ELT pipelines for structured and unstructured data.\nOptimize data storage, retrieval, and processing for performance, security, and cost-efficiency.\nEnsure data integrity and governance by implementing robust validation, monitoring, and compliance processes.\nConsume and analyze data from the data pipeline to infer, predict and recommend actionable insight, which will inform operational and strategic decision making to produce better results.\nEmpower departments and internal consumers with metrics and business intelligence to operate and direct our business, better serving our end customers.\nDetermine technical and behavioral requirements, identify strategies as solutions, and section solutions based on resource constraints.\nWork with the business, process owners, and IT team members to design solutions for data and advanced analytics solutions.\nPerform data modeling and prepare data in databases for analysis and reporting through various analytics tools.\nPlay a technical specialist role in championing data as a corporate asset.\nProvide technical expertise in collaborating with project and other IT teams, internal and external to the company.\nContribute to and maintain system data standards.\nResearch and recommend innovative, and where possible automated approaches for system data administration tasks. Identify approaches that leverage our resources and provide economies of scale.\nEngineer system that balances and meets performance, scalability, recoverability (including backup design), maintainability, security, high availability requirements and objectives.\n\n\nRequirements\n\nSkills:\n\n\nDatabricks and related – SQL, Python, PySpark, Delta Live Tables, Data pipelines, AWS S3 object storage, Parquet/Columnar file formats, AWS Glue.\nSystems Analysis - The application of systems analysis techniques and procedures, including consulting with users, to determine hardware, software, platform, or system functional specifications.\nTime Management - Managing one's own time and the time of others.\nActive Listening - Giving full attention to what other people are saying, taking time to understand the points being made, asking questions as appropriate, and not interrupting at inappropriate times.\nCritical Thinking - Using logic and reasoning to identify the strengths and weaknesses of alternative solutions, conclusions or approaches to problems.\nActive Learning - Understanding the implications of new information for both current and future problem-solving and decision-making.\nWriting - Communicating effectively in writing as appropriate for the needs of the audience.\nSpeaking - Talking to others to convey information effectively.\nInstructing - Teaching others how to do something.\nService Orientation - Actively looking for ways to help people.\nComplex Problem Solving - Identifying complex problems and reviewing related information to develop and evaluate options and implement solutions.\nTroubleshooting - Determining causes of operating errors and deciding what to do about it.\nJudgment and Decision Making - Considering the relative costs and benefits of potential actions to choose the most appropriate one.\n\n\nExperience And Education\n\n\nHigh School Diploma (or GED or High School Equivalence Certificate).\nAssociate degree or equivalent training and certification.\n5+ years of experience in data engineering including SQL, data warehousing, cloud-based data platforms.\n2+ years Project Lead or Supervisory experience preferred.\n\n\nIQ Ventures is an Equal Opportunity Employer. It’s our policy is not to discriminate against any applicant or employee based on actual or perceived race, age, sex or gender (including pregnancy), marital status, national origin, ancestry, citizenship status, mental or physical disability, religion, creed, color, sexual orientation, gender identity or expression (including transgender status), veteran status, genetic information, or any other characteristic protected by applicable federal, state or local law. We will provide accommodations to applicants needing accommodations to complete the application process.\n\nAt this time, IQ Ventures cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.\n\nIQ Ventures is an established, profitable technology and financial services company serving clients nationwide. Join the IQ Ventures team in our bright, modern Dublin, OH offices. IQ Ventures is well-regarded for its high integrity and collaborative leadership culture that rewards both individual thinking and team decision-making. Our leadership team is comprised of seasoned professionals who bring their vast experience and high standards of excellence to their work.\n\nWe are not accepting candidates from third-party recruiters at this time.\nShow more",
    "skills": [
      "aws",
      "data pipeline",
      "elt",
      "etl",
      "glue",
      "parquet",
      "pyspark",
      "python",
      "s3",
      "sql"
    ],
    "match_score": 0.9,
    "visa_sponsorship": false,
    "work_mode": "onsite"
  },
  {
    "job_id": "linkedin_4",
    "source": "linkedin",
    "job_title": "Data Engineer 1",
    "company_name": "ModMed",
    "location": "Boca Raton, FL",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-1-at-modmed-4335341809?position=4&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=LOJsYRt9JtC81Db%2FISPhZQ%3D%3D",
    "posted_date": "2025-11-10",
    "scraped_at": "2025-11-15T04:24:52.538810Z",
    "status": "new",
    "job_description": "We are united in our mission to make a positive impact on healthcare. Join Us!\n\n\nSouth Florida Business Journal, Best Places to Work 2024\nInc. 5000 Fastest-Growing Private Companies in America 2024\n2024 Black Book Awards, ranked #1 EHR in 11 Specialties\n2024 Spring Digital Health Awards, “Web-based Digital Health” category for EMA Health Records (Gold)\n2024 Stevie American Business Award (Silver), New Product and Service: Health Technology Solution (Klara)\n\n\nWho We Are\n\nWe Are Modernizing Medicine (WAMM)! We’re a team of bright, passionate, and positive problem-solvers on a mission to place doctors and patients at the center of care through an intelligent, specialty-specific cloud platform. Our vision is a world where the software we build increases medical practice success and improves patient outcomes. Founded in 2010 by Daniel Cane and Dr. Michael Sherling, we have grown to over 3400 combined direct and contingent team members serving eleven specialties, and we are just getting started! ModMed's global headquarters is based in Boca Raton, FL, with a growing office in Hyderabad, India, and a robust remote workforce across the US, Chile, and Germany.\n\nModMed is hiring an exceptional Data Engineer with a passion for technology to help revolutionize the world of Healthcare IT. Data engineers are at the core of a data-driven business; they build and maintain the infrastructure that empowers analysts and data scientists to drive insights. We’ve built a team of passionate, creative, and innovative engineers and data scientists that are changing the world and having fun doing it!\n\nYour Role\n\n\nCreate, maintain, and support ETL pipelines\nWork with application development teams to understand data representation\nMaintain documentation for the Data Warehouse and other data products\nSupport data usage needs of downstream analytics teams\n\n\nSkills & Requirements\n\n\nBS in Computer Science or equivalent work experience.\nProficient in SQL and relational databases.\nExperience with object-oriented programming.\nStrong problem solving skills, adaptable, proactive and willing to take ownership.\nStrong commitment to quality, architecture and documentation.\nExperience using AWS or other cloud services, a plus.\nExperience with big data tools (Hadoop, Spark, Kafka, Airflow), a plus.\nExperience with Python, Scala and R, a plus.\n\n\nPlease note: We are not offering sponsorship now or in the future for this role.\n\nModMed Benefits Highlight: At ModMed, we believe it’s important to offer a competitive benefits package designed to meet the diverse needs of our growing workforce. Eligible Modernizers can enroll in a wide range of benefits:\n\nIndia\n\n\nMeals & Snacks: Enjoy complimentary office lunches & dinners on select days and healthy snacks delivered to your desk,\nInsurance Coverage: Comprehensive health, accidental, and life insurance plans, including coverage for family members, all at no cost to employees,\nAllowances: Annual wellness allowance to support your well-being and productivity,\nEarned, casual, and sick leaves to maintain a healthy work-life balance,\nBereavement leave for difficult times and extended medical leave options,\nPaid parental leaves, including maternity, paternity, adoption, surrogacy, and abortion leave,\nCelebration leave to make your special day even more memorable, and company-paid holidays to recharge and unwind.\n\n\nUnited States\n\n\nComprehensive medical, dental, and vision benefits\n401(k): ModMed provides a matching contribution each payday of 50% of your contribution deferred on up to 6% of your compensation. After one year of employment with ModMed, 100% of any matching contribution you receive is yours to keep.\nGenerous Paid Time Off and Paid Parental Leave programs,\nCompany paid Life and Disability benefits, Flexible Spending Account, and Employee Assistance Programs,\nCompany-sponsored Business Resource & Special Interest Groups that provide engaged and supportive communities within ModMed,\nProfessional development opportunities, including tuition reimbursement programs and unlimited access to LinkedIn Learning,\nGlobal presence and in-person collaboration opportunities; dog-friendly HQ (US), Hybrid office-based roles and remote availability for some roles,\nWeekly catered breakfast and lunch, treadmill workstations, Zen, and wellness rooms within our BRIC headquarters.\n\n\nPHISHING SCAM WARNING: ModMed is among several companies recently made aware of a phishing scam involving imposters posing as hiring managers recruiting via email, text and social media. The imposters are creating misleading email accounts, conducting remote \"interviews,\" and making fake job offers in order to collect personal and financial information from unsuspecting individuals. Please be aware that no job offers will be made from ModMed without a formal interview process, and valid communications from our hiring team will come from our employees with a ModMed email address (first.lastname@modmed.com). Please check senders’ email addresses carefully. Additionally, ModMed will not ask you to purchase equipment or supplies as part of your onboarding process. If you are receiving communications as described above, please report them to the FTC website.\n\n\nShow more",
    "skills": [
      "airflow",
      "aws",
      "data warehouse",
      "etl",
      "kafka",
      "python",
      "spark",
      "spring",
      "sql"
    ],
    "match_score": 0.85,
    "visa_sponsorship": null,
    "work_mode": "remote"
  },
  {
    "job_id": "linkedin_15",
    "source": "linkedin",
    "job_title": "Data Engineer",
    "company_name": "PayPal",
    "location": "San Jose, CA",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-at-paypal-4338175128?position=15&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=dPvPc3d7U2%2BNfgmPrsUZMg%3D%3D",
    "posted_date": "2025-11-13",
    "scraped_at": "2025-11-15T04:24:53.083732Z",
    "status": "new",
    "job_description": "The Company\nPayPal has been revolutionizing commerce globally for more than 25 years. Creating innovative experiences that make moving money, selling, and shopping simple, personalized, and secure, PayPal empowers consumers and businesses in approximately 200 markets to join and thrive in the global economy.\n\nWe operate a global, two-sided network at scale that connects hundreds of millions of merchants and consumers. We help merchants and consumers connect, transact, and complete payments, whether they are online or in person. PayPal is more than a connection to third-party payment networks. We provide proprietary payment solutions accepted by merchants that enable the completion of payments on our platform on behalf of our customers.\n\nWe offer our customers the flexibility to use their accounts to purchase and receive payments for goods and services, as well as the ability to transfer and withdraw funds. We enable consumers to exchange funds more safely with merchants using a variety of funding sources, which may include a bank account, a PayPal or Venmo account balance, PayPal and Venmo branded credit products, a credit card, a debit card, certain cryptocurrencies, or other stored value products such as gift cards, and eligible credit card rewards. Our PayPal, Venmo, and Xoom products also make it safer and simpler for friends and family to transfer funds to each other. We offer merchants an end-to-end payments solution that provides authorization and settlement capabilities, as well as instant access to funds and payouts. We also help merchants connect with their customers, process exchanges and returns, and manage risk. We enable consumers to engage in cross-border shopping and merchants to extend their global reach while reducing the complexity and friction involved in enabling cross-border trade.\n\nOur beliefs are the foundation for how we conduct business every day. We live each day guided by our core values of Inclusion, Innovation, Collaboration, and Wellness. Together, our values ensure that we work together as one global team with our customers at the center of everything we do – and they push us to ensure we take care of ourselves, each other, and our communities.\n\nJob Description Summary:\nThe SMB & Financial Services (SMB&FS) Engineering team empowers small and medium-sized businesses to grow through PayPal’s trusted payment and commerce solutions. By helping merchants adopt our suite of Financial Services offerings—including Credit and Debit Cards, Business Loans, Buy Now Pay Later options, and Money Management tools—the team drives business expansion, operational efficiency, and customer success. These financial products also strengthen consumer engagement and loyalty by expanding how customers can pay, finance, and manage their money through PayPal. As a Data Engineer, you will design, build, and maintain scalable data pipelines and ETL processes that move and transform data from multiple sources into our data warehouse. Partnering closely with product managers, analysts, and other stakeholders, you’ll develop reliable, high-performance data solutions that enable business insights and data-driven decisions.\n\nJob Description:\nEssential Responsibilities:\n\n\nImplements tasks within the Software Development Lifecycle (SDLC), receiving structure and oversight from more experienced staff\nFollows well-established internal conventions and standard procedures\nUnderstands internal standards & processes an applies them to make technical decisions\nCollaborates with peers, manager, and project lead to gain understanding of tasks and review solutions\nMay contribute to code & design reviews\n\n\n\nExpected Qualifications:\n\n\n1+ years relevant experience and a Bachelor’s degree OR Any equivalent combination of education and experience.\n\n\n\nAdditional Responsibilities And Preferred Qualifications\nAdditional Responsibilities:\n\n\nEnsure data accuracy and integrity through automated data quality checks and validation processes.\nMonitor system performance, identify bottlenecks, and implement optimizations to improve scalability and efficiency.\nTroubleshoot and resolve data-related issues quickly to minimize impact on downstream systems.\nCreate and maintain clear documentation for pipeline design, workflows, and processes.\nParticipate in design and code reviews to uphold best practices and coding standards.\nStay current on emerging technologies and data engineering trends to continuously improve our data infrastructure.\n\n\n\nAdditional Qualifications:\n\n\nFamiliarity in SQL and scripting languages such as Python, with experience working with relational databases.\nFamiliarity in PySpark, Pandas or other data processing libraries.\nFamiliarity with data warehousing concepts and tools, such as AWS Redshift, Google BigQuery, or Snowflake, and experience optimizing performance for large-scale data processing.\nExperience with data modeling, schema design, and optimization techniques for scalability.\n\n\n\nPayPal is committed to fair and equitable compensation practices.\n\nActual Compensation is based on various factors including but not limited to work location, and relevant skills and experience.\n\nThe total compensation for this practice may include an annual performance bonus (or other incentive compensation, as applicable), equity, and medical, dental, vision, and other benefits. For more information, visit https://www.paypalbenefits.com.\n\nThe U.S. national annual pay range for this role is $100,500 to $173,250\n\nPayPal does not charge candidates any fees for courses, applications, resume reviews, interviews, background checks, or onboarding. Any such request is a red flag and likely part of a scam. To learn more about how to identify and avoid recruitment fraud please visit https://careers.pypl.com/contact-us.\n\nFor the majority of employees, PayPal's balanced hybrid work model offers 3 days in the office for effective in-person collaboration and 2 days at your choice of either the PayPal office or your home workspace, ensuring that you equally have the benefits and conveniences of both locations.\n\nOur Benefits:\n\nAt PayPal, we’re committed to building an equitable and inclusive global economy. And we can’t do this without our most important asset-you. That’s why we offer benefits to help you thrive in every stage of life. We champion your financial, physical, and mental health by offering valuable benefits and resources to help you care for the whole you.\n\nWe have great benefits including a flexible work environment, employee shares options, health and life insurance and more. To learn more about our benefits please visit https://www.paypalbenefits.com\n\nWho We Are:\n\nTo learn more about our culture and community visit https://about.pypl.com/who-we-are/default.aspx\n\nCommitment to Diversity and Inclusion\n\nPayPal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state, or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at paypalglobaltalentacquisition@paypal.com.\n\nBelonging at PayPal:\n\nOur employees are central to advancing our mission, and we strive to create an environment where everyone can do their best work with a sense of purpose and belonging. Belonging at PayPal means creating a workplace with a sense of acceptance and security where all employees feel included and valued. We are proud to have a diverse workforce reflective of the merchants, consumers, and communities that we serve, and we continue to take tangible actions to cultivate inclusivity and belonging at PayPal.\n\nAny general requests for consideration of your skills, please Join our Talent Community.\n\nWe know the confidence gap and imposter syndrome can get in the way of meeting spectacular candidates. Please don’t hesitate to apply.\n\nREQ ID R0132154\nShow more",
    "skills": [
      "aws",
      "data warehouse",
      "etl",
      "pandas",
      "pyspark",
      "python",
      "redshift",
      "sql"
    ],
    "match_score": 0.8,
    "visa_sponsorship": null,
    "work_mode": "hybrid"
  },
  {
    "job_id": "linkedin_12",
    "source": "linkedin",
    "job_title": "Data Engineer",
    "company_name": "Lowe's Companies, Inc.",
    "location": "Charlotte, NC",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-at-lowe-s-companies-inc-4298956736?position=12&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=nQmIJZMOCFpFCHOx6yxbNQ%3D%3D",
    "posted_date": null,
    "scraped_at": "2025-11-15T04:24:52.935376Z",
    "status": "new",
    "job_description": "Your Impact\n\nThe main purpose of this role is to build components and pipelines for delivering end-to-end data solutions for medium complex business problems along with the team. This role understands technical requirements and architecture and helps implement and maintain solutions as directed by more senior engineers. This role participates in developing the right solution, following best practices, and accounting for both functional and non-functional requirements, like reliability, scalability, performance, stability, security, and long-term maintainability.\n\nWork with a Winning Team\n\nAs part of a Fortune 500 company and retail leader, your work can change an entire industry. Our CEO is a forward-thinker when it comes to tech, and with one of Forbes Top 50 CIOs leading the charge, you can come to work knowing you’ll have access to the data, tools, and support that few other companies\n\ncan offer. We also know what it takes to create an inclusive culture that supports you. Our teams are structured around the engineer, giving you the support you need to do your best work. Since we’ve been in business for over 100 years, we’ve built an excellent track record of growth and\n\nsuccess. There’s peace of mind knowing you have the stability and resources you need to focus on solving tough challenges. And as you solve these challenges, know you’ll be surrounded by supportive associates with curious minds who listen to you, respect you, and recognize your hard work.\n\nInnovate in Charlotte\n\nThis position is based at our on-site at Lowe's Charlotte NC. Lowe's Tech Hub is an ultramodern work environment, complete with cutting-edge technology, collaborative workspaces, an on-site barista and Zen Garden, and other perks to enhance your work experience.\n\nWhat You Will Do\n\n\nDevelops scalable, flexible, and maintainable data and software solutions across business/enterprise applications, ensuring alignment with architectural standards and business requirements.\nDesigns and builds data pipelines (on-premises/cloud) to move, transform, and combine datasets from multiple systems, implementing SQL-based business metrics in collaboration with technical leads, analysts, and product owners.\nEnsures quality through unit/functional testing, supports UAT, and follows best practices in source control and CI/CD for efficient deployment.\nReviews technical designs, code, and documentation; participates in peer reviews and group design sessions to maintain team alignment.\nAnalyzes and organizes data to deliver consumable datasets, reports, and insights, while troubleshooting issues, supporting root cause analysis, and contributing to infrastructure and governance compliance.\n\n\nMinimum Qualifications\n\n\nBachelor's degree in engineering, computer science, computer information systems (CIS), or related field (or equivalent work experience in lieu of degree) and 2 years of experience in data, business intelligence, or platform engineering, data warehousing/ETL, or software engineering or equivalent experience\n2 years of expertise in object-oriented programming/structure programming, SQL, and scripting\n2 years of experience in big data technology and Cloud big data technologies\n1 year of experience working on project(s) involving the implementation of solutions applying development life cycles (SDLC)\n\n\nPreferred Skills/Education\n\n\nMaster's degree in computer science, CIS, or related field or equivalent experience\n2 years of expertise in Python, spark/dataproc, airflow or GCP data engineering\n\n\nAbout Lowe’s\n\nLowe’s Companies, Inc. (NYSE: LOW) is a FORTUNE® 500 home improvement company serving approximately 16 million customer transactions a week in the United States. With total fiscal year 2024 sales of more than $83 billion, Lowe’s operates over 1,700 home improvement stores and employs approximately 300,000 associates. Based in Mooresville, N.C., Lowe’s supports the communities it serves through programs focused on creating safe, affordable housing, improving community spaces, helping to develop the next generation of skilled trade experts and providing disaster relief to communities in need. For more information, visit Lowes.com.\n\nLowe’s is an equal opportunity employer and administers all personnel practices without regard to race, color, religious creed, sex, gender, age, ancestry, national origin, mental or physical disability or medical condition, sexual orientation, gender identity or expression, marital status, military or veteran status, genetic information, or any other category protected under federal, state, or local law.\n\nPay Range: $75,300.00 - $143,100.00 annually Starting rate of pay may vary based on factors including, but not limited to, position offered, location, education, training, and/or experience. For information regarding our benefit programs and eligibility, please visit https://talent.lowes.com/us/en/benefits.\nShow more",
    "skills": [
      "airflow",
      "ci/cd",
      "etl",
      "python",
      "spark",
      "sql"
    ],
    "match_score": 0.7,
    "visa_sponsorship": null,
    "work_mode": "onsite"
  },
  {
    "job_id": "linkedin_3",
    "source": "linkedin",
    "job_title": "Data Engineer( Python/streamlit/Snowflake)",
    "company_name": "Capgemini",
    "location": "New York, United States",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-python-streamlit-snowflake-at-capgemini-4335383096?position=3&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=YEAwePDnffmYFYDfa74L%2BQ%3D%3D",
    "posted_date": "2025-11-11",
    "scraped_at": "2025-11-15T04:24:52.489383Z",
    "status": "new",
    "job_description": "Choosing Capgemini means choosing a company where you will be empowered to shape your career in the way you’d like, where you’ll be supported and inspired by a collaborative community of colleagues around the world, and where you’ll be able to reimagine what’s possible. Join us and help the world’s leading organizations unlock the value of technology and build a more sustainable, more inclusive world.\n\nJob Description\n\nWe are seeking a skilled Open Source Developer with strong expertise in Python, Snowflake, and Streamlit to design, develop, and maintain scalable data-driven applications. The ideal candidate will have a passion for open-source technologies, strong problem-solving skills, and the ability to deliver high-quality solutions that support business intelligence and analytics initiatives.\n\nKey Responsibilities\n\n\nDevelop, test, and maintain Python applications with a focus on performance and scalability.\nDesign and optimize SQL queries and data models within Snowflake to ensure efficient data pipelines.\nBuild interactive Streamlit dashboards and data applications for business users and stakeholders.\nContribute to and leverage open-source libraries, ensuring code is reusable, modular, and well-documented.\nParticipate in code reviews, maintain version control (Git), and contribute to continuous integration and deployment processes.\n\n\nRequired Qualifications\n\n\nStrong proficiency in Python programming (data manipulation, API integration, and automation).\nHands-on experience with Snowflake (SQL, performance tuning, and warehouse management).\nProven ability to develop Streamlit dashboards and interactive data applications.\nFamiliarity with data modeling, ETL processes, and cloud-based data solutions (AWS, GCP, or Azure preferred).\n\n\nLife at Capgemini\n\nCapgemini Supports All Aspects Of Your Well-being Throughout The Changing Stages Of Your Life And Career. For Eligible Employees, We Offer\n\nFlexible work\n\nHealthcare including dental, vision, mental health, and well-being programs\n\nFinancial well-being programs such as 401(k) and Employee Share Ownership Plan\n\nPaid time off and paid holidays\n\nPaid parental leave\n\nFamily building benefits like adoption assistance, surrogacy, and cryopreservation\n\nSocial well-being benefits like subsidized back-up child/elder care and tutoring\n\nMentoring, coaching and learning programs\n\nEmployee Resource Groups\n\nDisaster Relief\n\nSalary Information\n\nCapgemini discloses salary range information in compliance with state and local pay transparency obligations. The disclosed range represents the lowest to highest salary we, in good faith, believe we would pay for this role at the time of this posting, although we may ultimately pay more or less than the disclosed range, and the range may be modified in the future. The disclosed range takes into account the wide range of factors that are considered in making compensation decisions including, but not limited to, geographic location, relevant education, qualifications, certifications, experience, skills, seniority, performance, sales or revenue-based metrics, and business or organizational needs. At Capgemini, it is not typical for an individual to be hired at or near the top of the range for their role. The base salary range for the tagged location is $103330 - $128656/yearly.\n\nThis role may be eligible for other compensation including variable compensation, bonus, or commission. Full time regular employees are eligible for paid time off, medical/dental/vision insurance, 401(k), and any other benefits to eligible employees.\n\nNote: No amount of pay is considered to be wages or compensation until such amount is earned, vested, and determinable. The amount and availability of any bonus, commission, or any other form of compensation that are allocable to a particular employee remains in the Company's sole discretion unless and until paid and may be modified at the Company’s sole discretion, consistent with the law.\n\nDisclaimer\n\nCapgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.\n\nThis is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.\n\nCapgemini is committed to providing reasonable accommodations during our recruitment process. If you need assistance or accommodation, please reach out to your recruiting contact.\n\nClick the following link for more information on your rights as an Applicant http://www.capgemini.com/resources/equal-employment-opportunity-is-the-law\n\nCapgemini is a global business and technology transformation partner, helping organizations to accelerate their dual transition to a digital and sustainable world, while creating tangible impact for enterprises and society. It is a responsible and diverse group of 340,000 team members in more than 50 countries. With its strong over 55-year heritage, Capgemini is trusted by its clients to unlock the value of technology to address the entire breadth of their business needs. It delivers end-to-end services and solutions leveraging strengths from strategy and design to engineering, all fueled by its market leading capabilities in AI, generative AI, cloud and data, combined with its deep industry expertise and partner ecosystem.\nShow more",
    "skills": [
      "aws",
      "etl",
      "git",
      "python",
      "sql"
    ],
    "match_score": 0.65,
    "visa_sponsorship": null,
    "work_mode": "onsite"
  },
  {
    "job_id": "linkedin_9",
    "source": "linkedin",
    "job_title": "Data Engineer (L5) - Games",
    "company_name": "Netflix",
    "location": "United States",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-l5-games-at-netflix-4313842380?position=9&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=ALLJY%2B%2FDPf4VlaaZ09yl3w%3D%3D",
    "posted_date": "2025-11-08",
    "scraped_at": "2025-11-15T04:24:52.788816Z",
    "status": "new",
    "job_description": "Netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.\n\nGames are our next big frontier and an incredible opportunity for us to deliver new experiences to delight and entertain our quickly growing membership. You will be joining us at an exciting time as we roll out more games on both mobile and cloud and be in a position to help us redefine what a Netflix subscription means for our members around the world.\n\nThe Data Science and Engineering (DSE) team uses data, analytics, and sciences to improve various aspects of our business. As a Senior Data Engineer focused on Core Metrics and Metadata, you will be a foundational partner, leveraging high-quality data to support end-to-end analytics needs for our Games partners.\n\nWhat You'll Do\n\n\nAs a Senior Data Engineer, your focus will be on architecting the foundation for all business measurements across our games portfolio on all platforms.\nOwn Core Metric Systems: Design, develop, and maintain the single source of truth for core business and operational metrics (e.g., DAU/MAU, retention, engagement rates). Ensure metric consistency, explainability, and reliability for high-stakes business decisions.\nData Modeling Architecture: Create advanced analytical data models (facts, dimensions, aggregations) optimized for performance, partitioning, and backfill considerations.\nImplement High-Reliability Pipelines: Build and optimize robust, high-volume data pipelines using distributed processing frameworks like Spark, Flink, Python, and Scala. Master complex processing patterns (batch, CDC, incremental loads, upserts) and address challenges like event-time partitioning, late data handling, and windowing for real-time applications.\nData Governance and Quality Leadership: Define and enforce data quality standards (accuracy, completeness, consistency, validity) at scale. Implement data lineage tracking and monitor metadata to provide the necessary context for interpreting core metrics.\nStrategic Collaboration: Partner with Games Product, Data Science, and Engineering stakeholders to translate their forecasting, research, and analytical needs into scalable data solutions. Collaborate on strategies for metric collection, dashboarding, and integration with analytical tools.\nTechnical Leadership: Proactively identify and resolve technical debt, increase automation, and champion best practices in data engineering and software development across the team. Mentor junior team members and help raise the technical bar for the entire DSE organization.\n\n\nWho You Are\n\nWe value people over process and look for humbly confident, action-oriented collaborators with deep technical expertise.\n\n\nExperience: 7+ years of hands-on experience in software development with a deep focus on building high-performance data systems that collect, process, and govern high-volume telemetry data for analytics.\nTechnical Expertise (The Core Domain):\nExpert-level Data Modeling: Proven mastery of telemetry (event schema design) and analytical modeling (dimensional modeling, aggregation strategies, partitioning considerations).\nDistributed Processing Mastery: Demonstrated experience building production-grade data pipelines using distributed processing frameworks (e.g., Spark, Flink, Hive/Hadoop), with a strong understanding of distributed systems and performance optimization.\nAdvanced Data Processing: Direct experience with complex batch and streaming data processing patterns, including CDC, incremental loads, event-driven architectures, and robust error handling.\nProgramming & SQL: Expert-level programming proficiency in Python and/or Scala/Java and mastery of SQL. You maintain a strong software engineering mindset.\nPartnership and Leadership:\nStrong Partnership Skills: Exceptional communication skills and a proven ability to collaborate effectively with non-technical stakeholders to ensure metrics are accessible, reliable, and actionable.\nProven ability to drive and own complex, cross-functional engineering projects with a high degree of autonomy.\nHumble confidence and the awareness to recognize when to course-correct, valuing continuous learning and improvement.\n\n\nOur compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.\n\nNetflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.\n\nInclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner.\n\nWe are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\n\nJob is open for no less than 7 days and will be removed when the position is filled.\nShow more",
    "skills": [
      "java",
      "python",
      "spark",
      "sql"
    ],
    "match_score": 0.6,
    "visa_sponsorship": null,
    "work_mode": "onsite"
  },
  {
    "job_id": "linkedin_1",
    "source": "linkedin",
    "job_title": "Analytics Engineer (L4) - Commerce Core",
    "company_name": "Netflix",
    "location": "Los Gatos, CA",
    "application_link": "https://www.linkedin.com/jobs/view/analytics-engineer-l4-commerce-core-at-netflix-4314930088?position=1&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=U0A0lsU2lemsFFgvK7IH3g%3D%3D",
    "posted_date": "2025-11-11",
    "scraped_at": "2025-11-15T04:24:52.383254Z",
    "status": "new",
    "job_description": "Netflix is one of the world's leading entertainment services, with over 300 million paid memberships in over 190 countries enjoying TV series, films and games across a wide variety of genres and languages. Members can play, pause and resume watching as much as they want, anytime, anywhere, and can change their plans at any time.\n\nAs Netflix's business grows, it becomes increasingly important to support a wide set of business stakeholders with the right data, metrics, and analytic tools. Our Commerce Core team within Finance Data Science and Engineering ensures we accurately measure Netflix’s business success, enabling our partners to access key metrics and leverage data efficiently to deepen their understanding of the business and optimize our business around the globe.\n\nWe are searching for an Analytics Engineer who is passionate about building scalable analytics solutions, developing metrics that drive executive decision-making, and experimenting with modern technologies. You will collaborate with cross-functional partners including Data Engineering, Finance, Product, and Engineering teams, to develop insights and tools that support critical business decisions and operational excellence at Netflix.\n\nIn This Role, You Will\n\n\nPartner with Data Engineers to design and implement robust data models and pipelines that enable richer, faster, and more accurate analysis.\nDesign, build, and maintain scalable, self-service analytics tools and data products to drive adoption across the business.\nDevelop, define, and maintain key metrics and dashboards for executive and business-critical reporting.\nProactively identify gaps and opportunities for innovation in analytics engineering, and drive initiatives to address them.\nExperiment with and evaluate new technologies, including Generative AI solutions, to improve analytics, reporting, and data accessibility.\nTranslate complex data insights and technical concepts into clear, actionable recommendations for technical and non-technical stakeholders.\n\n\nWe’d love to hear from you if you have:\n\n\nA degree in a quantitative field (e.g., mathematics, statistics, computer science, engineering, or similar), and 3+ years of experience in data engineering, data science, analytics engineering, or a similar data field.\nProficient in PySpark or Scala Spark, with experience building and scaling data models, pipelines, and analytics tools.\nAdvanced SQL skills, with experience working in complex data environments.\nExperience with scripting languages, particularly Python.\nFamiliarity with modern data technologies such as Trino, Spark, and Snowflake.\nUnderstanding of data governance, data lineage, and cataloging, and their importance for data quality and automation.\nExperience with data visualization platforms (e.g., Tableau, Looker, Plotly) and a strong ability to communicate data-driven stories.\nFamiliarity with Generative AI/ML concepts and a passion for experimenting with new technologies.\nExcellent verbal and written communication skills, with the ability to articulate metric definitions, data insights, and project roadmaps to diverse audiences.\nYou relate to and embody many aspects of Netflix's Culture. You love working independently, are comfortable with ambiguity, and have a passion for continuous learning.\nExperience in e-commerce or subscription-oriented businesses is a plus.\n\n\nOur compensation structure consists solely of an annual salary; we do not have bonuses. You choose each year how much of your compensation you want in salary versus stock options. To determine your personal top of market compensation, we rely on market indicators and consider your specific job family, background, skills, and experience to determine your compensation in the market range. The range for this role is $170,000 - $720,000.\n\nNetflix provides comprehensive benefits including Health Plans, Mental Health support, a 401(k) Retirement Plan with employer match, Stock Option Program, Disability Programs, Health Savings and Flexible Spending Accounts, Family-forming benefits, and Life and Serious Injury Benefits. We also offer paid leave of absence programs. Full-time hourly employees accrue 35 days annually for paid time off to be used for vacation, holidays, and sick paid time off. Full-time salaried employees are immediately entitled to flexible time off. See more detail about our Benefits here.\n\nNetflix has a unique culture and environment. Learn more here.\n\nInclusion is a Netflix value and we strive to host a meaningful interview experience for all candidates. If you want an accommodation/adjustment for a disability or any other reason during the hiring process, please send a request to your recruiting partner.\n\nWe are an equal-opportunity employer and celebrate diversity, recognizing that diversity builds stronger teams. We approach diversity and inclusion seriously and thoughtfully. We do not discriminate on the basis of race, religion, color, ancestry, national origin, caste, sex, sexual orientation, gender, gender identity or expression, age, disability, medical condition, pregnancy, genetic makeup, marital status, or military service.\nShow more",
    "skills": [
      "looker",
      "pyspark",
      "python",
      "spark",
      "sql",
      "tableau"
    ],
    "match_score": 0.55,
    "visa_sponsorship": null,
    "work_mode": "onsite"
  },
  {
    "job_id": "linkedin_14",
    "source": "linkedin",
    "job_title": "Data Engineer",
    "company_name": "Prestige Staffing",
    "location": "Houston, TX",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-at-prestige-staffing-4339934588?position=14&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=21I9SEt9TSLcLIcRgXGoUw%3D%3D",
    "posted_date": "2025-11-11",
    "scraped_at": "2025-11-15T04:24:53.033681Z",
    "status": "new",
    "job_description": "Job Title\n\nData Engineer\n\nPay\n\n$100,000 - $120,000 per year\n\nLocation\n\nHybrid – 5 Company Locations\n\nSummary\n\nWe are seeking a highly skilled Data Engineer to join a private-equity-backed company dedicated to innovative financial and operational data solutions. This role involves designing, implementing, and managing enterprise data warehouses with a focus on Oracle Database environments and Microsoft Fabric integrations. The successful candidate will support financial operations through advanced reporting, Power BI dashboards, and data modeling tailored to construction and project-focused industries such as commercial construction, with experience in % of completion, job costing, and cost accounting. This position offers a unique opportunity to work across multiple locations with a collaborative team, contributing to strategic decision-making and organizational growth.\n\nRequirements\n\n\nProven experience working with Oracle Database environments in data warehousing or enterprise data architecture roles\nProficiency with Microsoft Fabric and its data processing and analytics capabilities\nDomain knowledge in financial and construction-related data, including % of completion, contractor management, job costing, and cost accounting\nStrong skills in Power BI report/dashboard development and translating complex data into visual insights\nExpertise in ETL/ELT processes, data modeling, and warehousing best practices\nFamiliarity with Azure Data Warehouse platforms and SQL as a Service\nExperience in implementing data governance, security, and compliance standards\nExcellent problem-solving and stakeholder communication skills\n\n\nResponsibilities\n\n\nDesign, develop, and maintain scalable data warehouse solutions leveraging Oracle and Microsoft Fabric\nLead the development of ETL/ELT workflows for financial and project data, ensuring accuracy and integrity\nBuild and optimize Power BI dashboards to support strategic financial and operational decisions\nCollaborate with finance, operations, and IT teams to gather requirements and translate into technical solutions\nEstablish best practices for data governance, security, and compliance, especially around financial data\nManage data quality initiatives, metadata documentation, and process enhancements\nMentor and lead a team of data engineers and analysts, fostering a collaborative environment\nPartner with external consultants initially, then assume full ownership of the data systems at project completion\nMonitor system performance, optimize database performance, and ensure SLAs are met\n\n\nBenefits\n\n\nCompetitive benefits package with PE-backed growth opportunities\nPotential for career advancement in a rapidly expanding organization\nAccess to cutting-edge data technologies and frameworks\nCollaborative, dynamic work environment with multiple company locations\n\n\nShow more",
    "skills": [
      "data warehouse",
      "elt",
      "etl",
      "oracle",
      "power bi",
      "sql"
    ],
    "match_score": 0.55,
    "visa_sponsorship": null,
    "work_mode": "hybrid"
  },
  {
    "job_id": "linkedin_2",
    "source": "linkedin",
    "job_title": "Data Engineer",
    "company_name": "Prizeout",
    "location": "New York, NY",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-at-prizeout-4335471200?position=2&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=qWCkMXjYuuYgw%2BNPZE1tJw%3D%3D",
    "posted_date": "2025-11-11",
    "scraped_at": "2025-11-15T04:24:52.435558Z",
    "status": "new",
    "job_description": "What Is Prizeout\n\nPrizeout is a fast-growing fintech transforming how people interact with their money by turning everyday transactions into rewarding, value-driven moments. We sit at the intersection of payments, rewards, and loyalty, empowering financial institutions to build deeper connections with their users. Our platform analyzes rich transactional data to deliver personalized insights and maximize value with every purchase. The company was founded in 2019 and is headquartered in NYC.\n\nWhy Prizeout\n\nAt Prizeout, we’re redefining how people engage with their money through smarter, data-driven rewards. We’re looking for curious, analytical builders who love turning complex, messy data into meaningful insights that drive loyalty and better financial outcomes. We keep the user at the center of every decision and we value humility, curiosity, and ownership.\n\nAbout This Role\n\nAs a Data Engineer at Prizeout, you’ll own the data that powers CashBack+ and our credit union rewards ecosystem. Managing the end-to-end lifecycle of financial institution data, you’ll ensure every transaction is accurate, every action is recognized, and every reward is delivered on time. Working with core banking systems, card networks, and digital event data, you’ll turn complex, messy inputs into clean, reliable signals that drive real-time insights and loyalty programs.\n\nWhat You'll Do\n\n\nOwn and manage the full lifecycle of credit union core and card data pipelines\nBuild and maintain scalable pipelines that clean, standardize, enrich, and classify transactional and behavioral data\nTranslate raw transactions into meaningful insights using classification models (direct deposits, ACH events, MCC logic, etc.)\nDevelop and maintain automated validation and monitoring systems\nCreate and manage high-quality dashboards and reporting tools for credit unions and internal teams\nCollaborate with Product, Data Science, Operations, and Integrations teams to refine data models and onboarding processes\nContinuously improve data models, infrastructure, and internal tools\nStay current on fintech data architecture trends and emerging technologies to guide ongoing platform evolution\n\n\n\nWhat We're Looking For\n\n\n3+ years experience in software/data engineering, ideally in microservice or event-driven architecture\nAdvanced SQL and database design skills\nExperience with cloud environments and distributed systems\nExperience with financial/transaction data pipelines or interest in learning deeply\nFamiliarity with Snowflake or similar data warehouse tools (preferred)\nBias for action and comfort working with ambiguity\nAbility to thrive in a fast-moving, collaborative, mission-driven team\nClear communicator who brings clarity, not complexity\nScrappy, resilient, and energized by building things that work in the real world at scale\nPreference for candidates located in New York or Boston\n\n\n\nPlease Note:\n\n\nMust be authorized to work for any employer in the U.S. We are unable to sponsor or take over sponsorship of an employment visa at this time\n\n\n\nThe expected salary range for this position is $150,000-$170,000, plus an equity component.\n\nPrizeout is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.\n\nWe may use artificial intelligence (AI) tools to support parts of the hiring process, such as reviewing applications, analyzing resumes, or assessing responses. These tools assist our recruitment team but do not replace human judgment. Final hiring decisions are ultimately made by humans. If you would like more information about how your data is processed, please contact us.\nShow more",
    "skills": [
      "data warehouse",
      "sql"
    ],
    "match_score": 0.5,
    "visa_sponsorship": false,
    "work_mode": "onsite"
  },
  {
    "job_id": "linkedin_11",
    "source": "linkedin",
    "job_title": "Senior Finance Data Engineer / Data Analyst",
    "company_name": "Stellantis",
    "location": "Auburn Hills, MI",
    "application_link": "https://www.linkedin.com/jobs/view/senior-finance-data-engineer-data-analyst-at-stellantis-4335436204?position=11&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=vvP8btGbrLw%2FzkM5y9VZ1w%3D%3D",
    "posted_date": "2025-11-11",
    "scraped_at": "2025-11-15T04:24:52.889957Z",
    "status": "new",
    "job_description": "We are seeking a Finance Data Engineer / Data Analyst to support the FP&A organization by developing and maintaining Power BI-based financial reporting and analytics solutions. The ideal candidate will have strong data management and modeling skills, paired with a foundational understanding of financial concepts.\n\nResponsibilities Include\n\n\nManage the full Power BI data lifecycle: from data extraction and transformation (Bronze/Silver layers) to semantic modeling and dashboard development (Gold layer)\nEnsure data accuracy, performance, and security across all FP&A reporting tools\nHandle user access and role-level security within Power BI Service\n\n\nKey Skills & Requirements\n\n\nAdvanced proficiency in Power BI, Power Query, and DAX\nExperience integrating data from multiple sources (e.g., Excel, CSV, ERP systems like SAP or OneStream)\nAbility to clean, transform, and standardize data into analytics-ready models\nExperience deploying and maintaining dashboards in Power BI Service\nBasic understanding of financial reporting structures (e.g., P&L, revenue, cost, variance analysis)\nStrong analytical mindset with attention to data accuracy, consistency, and performance optimization\n\n\nBasic Qualifications\n\n\nBachelor’s degree in Data Analytics, Computer Science, Financial Technology or a related field\n5+ years extensive experience with PowerBI, 8+ years overall relevant work experience\nThrive in a fast-paced, dynamic work environment\n\n\nPreferred Qualifications\n\n\nPower BI certification or similar credentials\nExperience with ETL tools or scripting languages (e.g., SQL, Python) for advanced data transformation\nExperience with Onestream, SAP or Oracle tools\n\n\nOur Benefits — Designed With You In Mind\n\nComprehensive Health & Well-being Coverage\n\nFrom your very first day, you’ll have access to medical, dental, vision, and prescription drug coverage — ensuring you and your family stay healthy and protected. Also, our Employee Assistance Program (EAP) offers confidential support for personal and professional challenges, always ready when you need it.\n\nFamily Building Benefit\n\nWe proudly support all paths to parenthood- including fertility and infertility treatments, adoption services, and gestational surrogacy.\n\nGenerous Paid Time Off\n\nWe believe in work-life balance. That’s why we offer: 17+ paid holidays, including shut-down from December 24th through New Years Day every year. Vacation, float & wellbeing days, sick time and fully paid parental leave when your family needs you most.\n\nCompetitive Retirement Savings Plans\n\nWe Help You Plan For The Future With\n\n\nAn employer match on contributions to your 401k, Roth, and Catch-Up plans\nAn employer contribution, even if you don’t contribute\n\n\nIncome Protection & Insurance Options\n\nBenefit from both employer-provided and voluntary plan offerings, including life insurance, group accident, critical illness, etc. - supporting the needs of you and your family and ensuring peace of mind.\n\nCompany Vehicle Lease Program\n\nEligible employees and their immediate families can participate in the company vehicle lease program, providing access to Stellantis vehicles with insurance, maintenance, and unlimited miles included. Plus, take advantage of exclusive discounts on Stellantis products.\n\nSupport for Your Growth and Giving Back\n\nWe believe in investing in your future and your passions:\n\n\nTuition reimbursement\nStudent loan refinancing programs\n18 paid volunteer hours each year to make a difference in your community\n\n\nAnd so much more!\n\nWhen you join us, you’re not just building a career — you’re joining a company that supports you, inside and outside of work.\n\nAt Stellantis, we assess candidates based on qualifications, merit, and business needs. We welcome applications from all people without regard to sex, age, ethnicity, nationality, religion, sexual orientation, disability, or any characteristic protected by law. We believe that diverse teams reflect our identity as a global company, enabling us to better address the evolving needs of our customers and care for our future.\n\nEOE / Disability / Veteran\nShow more",
    "skills": [
      "csv",
      "etl",
      "oracle",
      "power bi",
      "powerbi",
      "python",
      "sql"
    ],
    "match_score": 0.5,
    "visa_sponsorship": null,
    "work_mode": "onsite"
  },
  {
    "job_id": "linkedin_8",
    "source": "linkedin",
    "job_title": "Data Engineer Intern (Summer 2026)",
    "company_name": "Lyft",
    "location": "San Francisco County, CA",
    "application_link": "https://www.linkedin.com/jobs/view/data-engineer-intern-summer-2026-at-lyft-4318160610?position=8&pageNum=0&refId=fxSFSpjJ6C1sX1ihn9dgGg%3D%3D&trackingId=o8jhECq3JQ5TuyVwE3Xe1w%3D%3D",
    "posted_date": "2025-11-12",
    "scraped_at": "2025-11-15T04:24:52.739736Z",
    "status": "new",
    "job_description": "At Lyft, our purpose is to serve and connect. We aim to achieve this by cultivating a work environment where all team members belong and have the opportunity to thrive.\n\nWe care deeply about delivering the best transportation experience; this means the best experience for the passenger and the best experience for the driver. We believe this quality of service can only be achieved with a deep understanding of our world, our cities, our streets… how they evolve, how they breathe.\n\nInterns work side-by-side with top engineers in the industry while having autonomy from the get-go. They contribute to user-facing products and are able to see their work go live quickly. Lyft fosters a collaborative environment in the office, so there's always a sharp mind eager to hear about your next idea. So what's yours?\n\nResponsibilities:\n\n\nOwn your project, while checking in with other team members throughout the day with questions and updates\nYou leave the code in a better state than when you found it (progressive refactor)\nYou value reliability, ensured by testing\nParticipate in code reviews to ensure code quality and distribute knowledge\nContinuous integration and deployment\nGo home knowing that your work today is meaningfully improving the lives of every Lyft driver and every Lyft passenger!\n\n\nExperience:\n\n\nCurrently pursuing a Bachelor's or Master's degree in Computer Science, Data Science or related major with a graduation date between December 2026 and Summer 2027 (required). For any candidates who are master's students who worked between their bachelor's and master's programs: candidates should also have less than 2 years of relevant full-time work experience\nAvailable during Summer 2026 for an internship in San Francisco\nStrong knowledge of CS fundamentals\nKnowledge of SQL and data modeling fundamentals\nExperience working with databases\nExcellent communication skills\nInterest in solving large scale data problems in a real world scenario\nPassion for community, sustainability, and/or transportation\n\n\nBenefits:\n\n\nGreat medical, dental, and vision insurance options\nMental health benefits\nIn addition to holidays, interns receive 2 days paid time off and 3 days sick time off\n401(k) plan to help save for your future\nSubsidized commuter benefits\nLyft Pink - Lyft team members get an exclusive opportunity to test new benefits of our Ridership Progra\n\n\nLyft is an equal opportunity employer committed to an inclusive workplace that fosters belonging. All qualified applicants will receive consideration for employment without regards to race, color, religion, sex, sexual orientation, gender identity, national origin, disability status, protected veteran status, age, genetic information, or any other basis prohibited by law. We also consider qualified applicants with criminal histories consistent with applicable federal, state and local law.\n\nLyft highly values having employees working in-office to foster a collaborative work environment and company culture. This role will be in-office on a hybrid schedule — Team Members will be expected to work in the office 3 days per week on Mondays, Wednesdays, and Thursdays. Lyft considers working in the office at least 3 days per week to be an essential function of this hybrid role. Your recruiter can share more information about the various in-office perks Lyft offers. #Hybrid\n\nThe expected range of pay for this position in the San Francisco Bay Area is $52-$58/hour. Salary ranges are dependent on a variety of factors, including qualifications, experience and geographic location. Your recruiter can share more information about the salary range specific to your working location and other factors during the hiring process.\nShow more",
    "skills": [
      "sql"
    ],
    "match_score": 0.05,
    "visa_sponsorship": null,
    "work_mode": "hybrid"
  }
]